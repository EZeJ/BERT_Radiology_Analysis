# BERT_Radiology_Analysis
In this paper, we will train and modify the original Transformers pre-trained model BERT on our radiology domain dataset for doing two main downstream tasks - Named Entity recognition task and examinations prediction.

# Abstract

The medical resources are becoming increasingly
scarce due to the pandemic and
the staff shortage of hospitals workers. Doctors
are facing more cases than they can
handle, which is causing missed diagnosis
and leading to more overtime working.
The raising of deep learning model of
NLP brings us the potential to solve this
task even if we don’t have the corresponding
medical domain linguistic knowledge.
In this paper, we will train and modify
the original Transformers pre-trained model
BERT on our radiology domain dataset for
doing two main downstream tasks - Named
Entity recognition task and examinations
prediction. We compared the performance
of BERT-base model with whether trained
or not trained tokenizer on a publicly NER
dataset. And we also compare the result
with other people’s medical BERT model
on the same dataset. On the examinations
prediction task, we tried two different methods.
Those are multi-class and multi-label
prediction BERT model. Overall, we got an
74% accuracy by the multi-label model and
59% by the multi-class model. In the meantime,
the normal BERT model only has 42%
accuracy.


